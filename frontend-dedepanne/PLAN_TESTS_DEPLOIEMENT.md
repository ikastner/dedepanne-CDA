# Activit√©-type n¬∞3 : Pr√©parer le d√©ploiement d'une application s√©curis√©e

## Exemple n¬∞1 : Pr√©parer et ex√©cuter les plans de tests d'une application

### 1. D√©crivez les t√¢ches ou op√©rations que vous avez effectu√©es, et dans quelles conditions :

Avant de d√©ployer mon application D√©d√©panne en production, j'ai pr√©par√© et ex√©cut√© un plan de tests complet afin de m'assurer que toutes les fonctionnalit√©s d√©velopp√©es r√©pondent correctement aux besoins et qu'aucun bug majeur ne subsiste. Voici les √©tapes que j'ai men√©es :

#### ‚Ä¢ D√©finition des sc√©narios de test √† partir des sp√©cifications :
J'ai relu le PRD et les user stories du projet pour en extraire tous les cas d'utilisation √† v√©rifier. J'ai list√© par √©crit chaque fonctionnalit√© et imagin√© les sc√©narios de test associ√©s. Par exemple : "Cr√©ation d'une demande de r√©paration r√©ussie" (chemin nominal o√π un client planifie une intervention et tout fonctionne), "Cr√©ation d'une demande de r√©paration ‚Äì cas date indisponible" (message d'erreur si le cr√©neau demand√© est d√©j√† occup√©), "Connexion ‚Äì mot de passe incorrect" (v√©rifier le message d'erreur et l'absence d'acc√®s), etc. Pour chaque fonctionnalit√© (prise de rendez-vous, gestion du stock de produits reconditionn√©s, don d'appareil, etc.), j'ai envisag√© le chemin nominal et au moins un chemin alternatif/erreur. Au total, j'ai identifi√© plusieurs dizaines de sc√©narios de tests couvrant l'ensemble du p√©rim√®tre.

#### ‚Ä¢ √âlaboration du plan de test (documentation) :
J'ai formalis√© ces sc√©narios dans un plan de test structur√©. Dans un tableau, j'ai cr√©√© une ligne par cas de test avec : un identifiant, l'objectif du test, les pr√©requis (√©tat du syst√®me avant test, ex : "un compte client existant"), les √©tapes pr√©cises de r√©alisation (ex : "1. Aller sur la page d'accueil, 2. Cliquer sur 'Nouvelle r√©paration', 3. Remplir le formulaire‚Ä¶") et le r√©sultat attendu pour chaque √©tape ou en final. Par exemple, pour le test "Annulation de r√©paration par le client", le r√©sultat attendu final √©tait "La r√©paration passe au statut 'Annul√©e' et n'appara√Æt plus dans la liste active du client, un email de confirmation est envoy√©". J'ai pris soin d'√©crire ce plan de test de mani√®re claire, comme si une autre personne devait l'ex√©cuter.

#### ‚Ä¢ Pr√©paration des donn√©es et de l'environnement de test :
J'ai cr√©√© un environnement de pr√©-production identique √† la prod cible (base de donn√©es d√©di√©e, configuration identique) et j'y ai d√©ploy√© la derni√®re version de l'application. J'ai √©galement pr√©par√© des donn√©es de test coh√©rentes : par exemple, j'ai cr√©√© via des scripts SQL quelques comptes clients fictifs, dont un "client de d√©monstration" qui servirait pour les tests de parcours client, et des enregistrements correspondants (quelques r√©parations d√©j√† planifi√©es, quelques produits reconditionn√©s en stock, etc.). Ceci pour √©viter de partir d'une base totalement vide, ce qui ne refl√©terait pas la r√©alit√© d'utilisation. J'ai document√© l'√©tat initial de la base de test pour chaque sc√©nario si n√©cessaire (par ex., pour tester la modification d'un stock, il fallait qu'il y ait d√©j√† un stock existant).

#### ‚Ä¢ Ex√©cution des tests fonctionnels :
J'ai ensuite d√©roul√© manuellement chaque cas de test du plan sur l'application d√©ploy√©e en pr√©-production. Concr√®tement, j'ai simul√© le comportement d'un utilisateur final : navigation via un navigateur web sur le front-end Next.js, utilisation de l'application comme un client ou comme le technicien selon le sc√©nario, et observation des r√©sultats. J'ai coch√© au fur et √† mesure les √©tapes r√©ussies et not√© toute divergence. Par exemple, lors du test de "filtre des r√©parations par date", j'ai constat√© qu'un filtre ne fonctionnait pas correctement sur la plage de dates ‚Äì un bug que j'ai imm√©diatement consign√©. √Ä chaque test, je v√©rifiais non seulement l'interface visible, mais aussi l'impact en base de donn√©es (via pgAdmin) pour les op√©rations critiques : par exemple, apr√®s un test de cr√©ation de r√©paration, je v√©rifiais que l'enregistrement correspondant apparaissait bien en base avec les bons attributs, et que le log √©tait cr√©√© en MongoDB.

#### ‚Ä¢ Identification et suivi des anomalies :
Chaque √©cart entre le r√©sultat attendu et le r√©sultat observ√© a √©t√© not√© comme une anomalie. J'ai tenu un fichier de suivi des bugs rencontr√©s durant la campagne de test. Pour chacun, j'ai not√© : description du probl√®me, √©tapes pour le reproduire, gravit√© (bloquant, majeur, mineur), et si possible j'ai tent√© d'analyser la cause. Par exemple, j'ai d√©couvert un bug o√π la suppression d'un produit reconditionn√© ne rafra√Æchissait pas automatiquement la liste √† l'√©cran ‚Äì bug mineur d'UX. En revanche, un bug plus grave trouv√© : l'application permettait de planifier deux r√©parations √† la m√™me date et m√™me heure pour le technicien (conflit d'agenda non d√©tect√©). Je l'ai qualifi√© de majeur car cela pourrait arriver en prod et causer un vrai probl√®me d'organisation. J'ai ensuite corrig√© imm√©diatement les anomalies les plus simples (ce qui a donn√© lieu √† un patch d√©ploy√© sur l'environnement de test), et planifi√© de corriger les plus complexes. J'ai rejou√© les tests sur les parties corrig√©es pour m'assurer que le correctif √©tait bon.

En compl√©ment des tests purement fonctionnels, j'ai proc√©d√© √† quelques tests de s√©curit√© basiques dans le cadre de cette pr√©paration. Par exemple, j'ai tent√© des entr√©es malveillantes dans les formulaires (injection de code `<script>` dans un champ de description, tr√®s long texte pour tester la robustesse, caract√®res sp√©ciaux inattendus) afin de v√©rifier que l'application ne les acceptait pas ou les g√©rait correctement. J'ai aussi v√©rifi√© les restrictions d'acc√®s : en me connectant en tant que client, j'ai tent√© d'acc√©der √† des pages r√©serv√©es au technicien (URL directes), et l'application m'a bien renvoy√© une page de refus (statut 403) ‚Äì preuve que le syst√®me d'authentification et d'autorisation √©tait effectif. Ces tests m'ont rassur√© sur le fait que les principaux garde-fous de s√©curit√© √©taient en place.

### 2. Pr√©cisez les moyens utilis√©s :

#### ‚Ä¢ Documentation de test :
Un fichier Excel (ou Google Sheets) a √©t√© utilis√© pour r√©diger le plan de test et consigner les r√©sultats. Chaque ligne correspondait √† un cas de test, avec des colonnes pour les √©tapes, les r√©sultats attendus et obtenus, et un statut (Succ√®s/√âchec). Ce support m'a permis de garder une trace √©crite et exploitable de l'avanc√©e de mes tests.

#### ‚Ä¢ Environnement de pr√©-production :
Une machine virtuelle Linux sur laquelle l'application a √©t√© d√©ploy√©e, avec une configuration identique √† l'environnement de production vis√© (m√™me version de PostgreSQL, de Node.js, variables d'environnement de test). Cet environnement a servi de bac √† sable pour ex√©cuter les plans de test en conditions quasi r√©elles sans risque pour les donn√©es de production (inexistantes √† ce stade, puisque pas encore d√©ploy√© chez le client).

#### ‚Ä¢ Outils de test manuel :
Principal outil = le navigateur web (Chrome et Firefox) pour tester l'interface utilisateur c√¥t√© client et c√¥t√© technicien. J'ai √©galement utilis√© mon smartphone pour v√©rifier l'affichage responsive et le comportement en mobile (puisque l'application devait √™tre utilisable sur le terrain par le technicien via son t√©l√©phone). Pour les tests d'API plus techniques, j'ai fait appel √† Postman ‚Äì par exemple, pour simuler un appel API direct en cas d'utilisation future d'une appli mobile, ou pour tester certaines validations serveurs sans passer par l'UI.

#### ‚Ä¢ Outils de suivi des anomalies :
Un second onglet dans mon fichier Excel listait les anomalies identifi√©es. Cependant, pour faire les choses proprement, j'ai aussi exp√©riment√© l'outil Azure DevOps (Boards) en mode solo : j'ai cr√©√© des Work Items de type Bug pour chaque anomalie notable, histoire de m'habituer √† cet usage. M√™me si je suis seul, √ßa m'a servi √† prioriser les corrections (j'ai attribu√© des niveaux de s√©v√©rit√© et j'ai tri√© la liste pour m'attaquer d'abord aux bloquants).

#### ‚Ä¢ Outils compl√©mentaires :
J'ai mis en place une strat√©gie de tests automatis√©s compl√®te pour garantir la qualit√© de l'application. Pour les tests E2E, j'ai utilis√© **Playwright** qui permet d'automatiser les tests sur plusieurs navigateurs (Chrome, Firefox, Safari) et appareils mobiles. J'ai cr√©√© des scripts de test automatis√©s pour tous les flux critiques : authentification, cr√©ation de demandes de r√©paration, achat de produits reconditionn√©s, et dons d'appareils. Ces tests s'ex√©cutent automatiquement dans le pipeline CI/CD via GitHub Actions.

Pour les tests frontend, j'ai utilis√© **Jest** avec **React Testing Library** pour tester les composants React de mani√®re isol√©e, en simulant les interactions utilisateur et en v√©rifiant le comportement attendu. J'ai √©galement mis en place des tests unitaires backend avec Jest pour les services NestJS, couvrant l'authentification, la gestion des r√©parations, et les validations m√©tier.

En compl√©ment, j'ai utilis√© les **DevTools du navigateur** pour analyser les erreurs JavaScript et les √©changes r√©seau en temps r√©el lors du d√©veloppement et du debug des tests. Cette approche automatis√©e m'a permis de d√©tecter rapidement les r√©gressions et de maintenir une qualit√© √©lev√©e du code.

---

## üìã PLAN DE TESTS D√âTAILL√â - D√âD√âPANNE

### **Tests d'Authentification**

| ID | Test | Pr√©requis | √âtapes | R√©sultat Attendu |
|---|---|---|---|---|
| AUTH-001 | Connexion r√©ussie | Compte client existant | 1. Aller sur /login<br>2. Saisir email/password valides<br>3. Cliquer "Se connecter" | Redirection vers dashboard, token JWT cr√©√© |
| AUTH-002 | Connexion √©chec - mauvais mot de passe | Compte client existant | 1. Aller sur /login<br>2. Saisir email correct + mauvais password<br>3. Cliquer "Se connecter" | Message d'erreur, pas de redirection |
| AUTH-003 | Inscription nouveau client | Aucun | 1. Aller sur /register<br>2. Remplir formulaire complet<br>3. Valider | Compte cr√©√©, email de confirmation |
| AUTH-004 | Acc√®s page admin en tant que client | Compte client connect√© | 1. Se connecter en tant que client<br>2. Aller sur /admin | Redirection 403, message d'erreur |

### **Tests de R√©parations**

| ID | Test | Pr√©requis | √âtapes | R√©sultat Attendu |
|---|---|---|---|---|
| REP-001 | Cr√©ation demande r√©paration | Client connect√© | 1. Aller sur /services<br>2. Remplir formulaire r√©paration<br>3. Valider | Demande cr√©√©e, email de confirmation |
| REP-002 | Planification intervention | Demande r√©paration existante | 1. Technicien connect√©<br>2. S√©lectionner cr√©neau disponible<br>3. Confirmer | Statut "planifi√©", notification client |
| REP-003 | Conflit d'agenda | Deux r√©parations m√™me cr√©neau | 1. Planifier r√©paration 1<br>2. Tenter r√©paration 2 m√™me cr√©neau | Message d'erreur, cr√©neau non disponible |
| REP-004 | Annulation par client | R√©paration planifi√©e | 1. Client connect√©<br>2. Aller sur ses r√©parations<br>3. Annuler | Statut "annul√©e", email confirmation |

### **Tests de Produits Reconditionn√©s**

| ID | Test | Pr√©requis | √âtapes | R√©sultat Attendu |
|---|---|---|---|---|
| PROD-001 | Consultation catalogue | Aucun | 1. Aller sur /reconditioned<br>2. Parcourir les produits | Liste des produits avec filtres |
| PROD-002 | Filtrage par cat√©gorie | Catalogue avec produits | 1. S√©lectionner cat√©gorie "Lave-linge"<br>2. Appliquer filtre | Seuls les lave-linges affich√©s |
| PROD-003 | Achat produit | Client connect√© + produit en stock | 1. S√©lectionner produit<br>2. Ajouter au panier<br>3. Finaliser commande | Commande cr√©√©e, stock d√©cr√©ment√© |
| PROD-004 | Produit hors stock | Produit avec stock = 0 | 1. Tenter d'acheter produit | Message "indisponible", bouton d√©sactiv√© |

### **Tests de Dons d'Appareils**

| ID | Test | Pr√©requis | √âtapes | R√©sultat Attendu |
|---|---|---|---|---|
| DON-001 | √âvaluation appareil | Aucun | 1. Aller sur /donations<br>2. Remplir formulaire √©valuation<br>3. Soumettre | Estimation prix, bon d'achat g√©n√©r√© |
| DON-002 | Planification collecte | Don valid√© | 1. Client connect√©<br>2. Choisir cr√©neau collecte<br>3. Confirmer | Collecte planifi√©e, email confirmation |
| DON-003 | Suivi reconditionnement | Appareil collect√© | 1. Admin connect√©<br>2. Mettre √† jour statut<br>3. Notifier client | Statut mis √† jour, notification client |

### **Tests de S√©curit√©**

| ID | Test | Pr√©requis | √âtapes | R√©sultat Attendu |
|---|---|---|---|---|
| SEC-001 | Injection SQL | Formulaire de recherche | 1. Saisir `' OR 1=1 --`<br>2. Soumettre | Erreur 400, pas d'ex√©cution SQL |
| SEC-002 | XSS | Champ description | 1. Saisir `<script>alert('test')</script>`<br>2. Sauvegarder | Script √©chapp√©, pas d'ex√©cution |
| SEC-003 | CSRF | Session active | 1. Cr√©er requ√™te malveillante<br>2. Tenter ex√©cution | Token CSRF requis, requ√™te rejet√©e |
| SEC-004 | Rate limiting | API endpoints | 1. Envoyer 100 requ√™tes/min<br>2. Continuer | Limitation activ√©e, erreur 429 |

### **Tests de Performance**

| ID | Test | Pr√©requis | √âtapes | R√©sultat Attendu |
|---|---|---|---|---|
| PERF-001 | Temps de r√©ponse API | Serveur en charge | 1. Mesurer temps r√©ponse<br>2. Comparer baseline | < 200ms pour 95% des requ√™tes |
| PERF-002 | Charge base donn√©es | 1000 utilisateurs simultan√©s | 1. Simuler charge<br>2. Monitorer PostgreSQL | Pas de timeout, < 80% CPU |
| PERF-003 | Upload images | Fichiers 5MB | 1. Upload 10 images<br>2. Mesurer temps | < 30s total, compression OK |

### **Tests d'Interface Utilisateur**

| ID | Test | Pr√©requis | √âtapes | R√©sultat Attendu |
|---|---|---|---|---|
| UI-001 | Responsive design | Navigateur mobile | 1. Tester sur mobile<br>2. V√©rifier adaptation | Interface adapt√©e, navigation OK |
| UI-002 | Accessibilit√© | Lecteur d'√©cran | 1. Tester avec NVDA<br>2. V√©rifier contrastes | Conforme WCAG 2.1 AA |
| UI-003 | Navigation | Utilisateur connect√© | 1. Tester tous les liens<br>2. V√©rifier breadcrumbs | Navigation fluide, pas de 404 |

---

## üõ†Ô∏è OUTILS DE TEST UTILIS√âS

### **Environnement de Test**
- **Docker Compose** : Environnement complet (PostgreSQL + MongoDB + Backend + Frontend)
- **Base de donn√©es de test** : Donn√©es fictives coh√©rentes
- **Variables d'environnement** : Configuration s√©par√©e dev/test/prod

### **Outils de Test**
- **Postman** : Tests API REST
- **Chrome DevTools** : Debug frontend, monitoring r√©seau
- **pgAdmin** : V√©rification base PostgreSQL
- **MongoDB Compass** : V√©rification logs MongoDB
- **Jest** : Tests unitaires backend et frontend
- **React Testing Library** : Tests de composants React
- **Playwright** : Tests E2E automatis√©s multi-navigateurs
- **Lighthouse** : Performance et accessibilit√©

### **Monitoring**
- **Prometheus** : M√©triques temps r√©el
- **Grafana** : Dashboards de monitoring
- **Winston** : Logs structur√©s
- **Sentry** : Tracking d'erreurs (optionnel)

---

## üìä M√âTRIQUES DE QUALIT√â

### **Couverture de Tests**
- **Tests unitaires** : > 80% de couverture
- **Tests d'int√©gration** : Tous les endpoints API
- **Tests fonctionnels** : 100% des user stories
- **Tests de s√©curit√©** : OWASP Top 10

### **Performance**
- **Temps de r√©ponse** : < 200ms (95e percentile)
- **Disponibilit√©** : > 99.9%
- **Taux d'erreur** : < 0.1%
- **Score Lighthouse** : > 90

### **S√©curit√©**
- **Vuln√©rabilit√©s critiques** : 0
- **Vuln√©rabilit√©s majeures** : 0
- **Conformit√© RGPD** : 100%
- **Chiffrement** : TLS 1.3 partout

---

**D√©d√©panne - Tests de d√©ploiement s√©curis√©** üîí‚úÖ 